# Vision Agent ğŸ¬ğŸ§ 

![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Python 3.11](https://img.shields.io/badge/Python-3.11-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688.svg)
![Docker](https://img.shields.io/badge/Docker-ready-2496ED.svg)

**Real-time multimodal AI video agent** that watches, listens, and understands video â€” built for the [WeMakeDevs Vision Possible Hackathon](https://wemakedevs.org).

> Upload or stream video â†’ Extract frames + Transcribe audio + Detect objects â†’ Generate AI-powered study notes, formulas, and viva questions â€” all in real-time.

---

## âœ¨ Features

| Feature | Detail |
|---|---|
| ğŸ“¤ **Video Upload** | Drag-and-drop, supports MP4/MOV/WebM |
| ğŸ–¼ï¸ **Frame Extraction** | 1 fps sampling via OpenCV |
| ğŸ™ï¸ **Audio Transcription** | OpenAI Whisper API (cloud) |
| ğŸ” **Object Detection** | YOLOv8 per-frame labels with confidence |
| ğŸ§  **AI Notes** | LLM-generated summary, concepts, formulas, viva questions |
| ğŸ’¬ **Ask a Question** | Contextual QA chat over notes + transcript |
| ğŸ§ª **Quiz Generator** | MCQs + short-answer questions with auto-scoring |
| ğŸ“ **LaTeX Formulas** | MathJax-rendered formulas extracted from lectures |
| ğŸ“¡ **Live Streaming** | Webcam streaming in 2s chunks with real-time agent responses |
| ğŸ–¼ï¸ **Timeline Thumbnails** | Clickable frame timeline for video navigation |
| âŒ¨ï¸ **Keyboard Shortcuts** | Space play/pause, arrow keys Â±5s |
| âš¡ **LLM Caching** | In-memory cache for fast repeated queries |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser UI  â”‚â”€â”€â”€â–¶â”‚              FastAPI Server                  â”‚
â”‚              â”‚    â”‚                                              â”‚
â”‚  Upload tab  â”‚    â”‚  /upload â”€â”€â–¶ frame_extractor.py              â”‚
â”‚  Demo page   â”‚    â”‚  /analyze â”€â”€â–¶ ffmpeg audio â”€â”€â–¶ whisper       â”‚
â”‚  Stream tab  â”‚    â”‚              â”€â”€â–¶ YOLOv8 detection            â”‚
â”‚              â”‚    â”‚  /generate_notes â”€â”€â–¶ OpenAI LLM              â”‚
â”‚  QA Chat     â”‚    â”‚  /ask â”€â”€â–¶ contextual QA (cached)             â”‚
â”‚  Quiz Modal  â”‚    â”‚  /generate_quiz â”€â”€â–¶ MCQ + short answer       â”‚
â”‚              â”‚    â”‚  /stream_chunk â”€â”€â–¶ instant per-chunk agent    â”‚
â”‚              â”‚    â”‚  /stream_finalize â”€â”€â–¶ stitch + full analyze   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Option A: Docker (recommended â€” no ffmpeg install needed)

```bash
# Set your API key and run
OPENAI_API_KEY="sk-..." docker compose up --build
# Open http://localhost:8000
```

### Option B: Local Setup

#### Prerequisites

- **Python 3.10+**
- **ffmpeg** installed and on PATH ([download](https://ffmpeg.org/download.html))
- **OpenAI API key** (for transcription, notes, QA, and quiz)

#### Windows PowerShell

```powershell
cd vision-agent\backend
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
$env:OPENAI_API_KEY = "sk-..."
uvicorn main:app --reload --port 8000
```

#### Linux / macOS

```bash
cd vision-agent/backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY="sk-..."
uvicorn main:app --reload --port 8000
```

Open **http://localhost:8000** â€” Upload & Analyze UI
Open **http://localhost:8000/demo** â€” Interactive Demo (QA, Quiz, Timeline)

> **ğŸ’¡ No API key?** The server runs without `OPENAI_API_KEY` â€” transcription returns a placeholder and notes serve pre-generated samples from `analysis/sample/`. Judges can browse the demo UI and sample outputs immediately.

## ğŸ“¡ API Endpoints

| Method | Path | Description |
|---|---|---|
| GET | `/` | Upload & stream UI |
| GET | `/demo` | Interactive demo (QA, quiz, timeline) |
| POST | `/upload` | Upload video â†’ extract frames |
| POST | `/analyze` | Full pipeline: frames + transcript + detection |
| POST | `/generate_notes?video_stem=video` | LLM notes from analysis |
| POST | `/ask` | Contextual QA over notes + transcript |
| POST | `/generate_quiz` | MCQ + short-answer quiz from notes |
| POST | `/stream_chunk` | Stream a 2-5s chunk for instant processing |
| POST | `/stream_finalize` | Stitch chunks + run full analysis |

## âš™ï¸ Environment Variables

| Variable | Default | Description |
|---|---|---|
| `OPENAI_API_KEY` | â€” | **Required** for transcription, notes, QA, quiz |
| `WHISPER_MODEL` | `tiny` | Whisper model size |
| `YOLO_MODEL` | `yolov8n.pt` | YOLO model file |
| `LLM_MODEL` | `gpt-4o-mini` | OpenAI chat model |

## ğŸ› ï¸ Tech Stack

- **Backend**: Python 3.11, FastAPI, Uvicorn
- **Vision**: OpenCV, YOLOv8 (ultralytics)
- **Audio**: OpenAI Whisper API
- **LLM**: OpenAI GPT-4o-mini
- **Math**: MathJax 3 (LaTeX rendering)
- **Streaming**: ffmpeg, MediaRecorder API
- **Frontend**: Vanilla HTML/CSS/JS â€” dark glassmorphism
- **Deploy**: Docker, GitHub Actions CI

## ğŸ“ Project Structure

```
vision-agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app (all routes)
â”‚   â”œâ”€â”€ frame_extractor.py   # OpenCV frame extraction
â”‚   â”œâ”€â”€ transcribe.py        # OpenAI Whisper API transcription
â”‚   â”œâ”€â”€ detect.py            # YOLOv8 detection
â”‚   â”œâ”€â”€ llm_helpers.py       # LLM call wrapper with retry
â”‚   â”œâ”€â”€ generate_notes.py    # Notes generator (with fallback)
â”‚   â”œâ”€â”€ streaming.py         # Real-time chunk streaming
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ final_test_report.txt
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ analysis/sample/     # Pre-generated sample outputs
â”‚   â”‚   â”œâ”€â”€ analysis.json
â”‚   â”‚   â”œâ”€â”€ notes.json
â”‚   â”‚   â””â”€â”€ quiz.json
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html       # Upload & live-stream UI
â”‚       â””â”€â”€ demo.html        # Interactive demo (QA, quiz, timeline)
â”œâ”€â”€ .github/workflows/ci.yml # GitHub Actions CI
â”œâ”€â”€ Dockerfile               # Docker build
â”œâ”€â”€ docker-compose.yml       # One-command start
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE                  # MIT
â”œâ”€â”€ PRIVACY.md               # Data handling note
â”œâ”€â”€ BLOG_POST.md             # Blog draft
â”œâ”€â”€ SUBMISSION_NOTES.md      # Metrics & pitch
â”œâ”€â”€ SUBMISSION_READY.txt     # Hackathon form fields
â””â”€â”€ RELEASE_NOTES.md         # GitHub Release notes
```

## ğŸ“Š Performance Metrics

| Step | Time |
|---|---|
| Frame extraction (30s video) | ~1-2s |
| Whisper transcription (cloud) | ~2-5s |
| YOLOv8 detection (30 frames) | ~3-6s |
| LLM notes generation | ~3-8s |
| **Total pipeline** | **~10-20s** |

## License

MIT â€” see [LICENSE](LICENSE)

---

Built with â¤ï¸ for the **WeMakeDevs Vision Possible Hackathon** â€” powered by [Vision Agents by Stream](https://getstream.io/video/vision-agents/) & [OpenAI](https://openai.com)

